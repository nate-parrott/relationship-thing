CLASSIFIERS
-----------------

<p>To start building the classifier, we needed to modify the data we collected. Some of the features we wished to test weren't binary to start with. For example, "age difference" was a feature we wished to include. This started out as a range of numbers and we were able to convert it into 0 or 1 depending on the value. If the age difference between partners was less than 5 years the value became 0. If the age difference was greater than or equal to 5 years, the value became 1.
</p>

<p>Another issue we ran into was missing data. Some of the interviewees refused to answer certain questions, so there were plenty of "observation" with a missing feature. Instead of throwing the data out, we decided to do some research on how to best deal with the problem. We settled on assigning that a missing feature randomly to 0 or 1. Other options were computationally intensive and seemed unnecessary.
</p>

<p>
Finally, with a set of observations composed of features and a set of observation labels indicating if that couple was still together or not, we could start building classifiers. We decided to test four different classifiers:
	  <ul>
		<li>Decision Tree</li>
		<li>Naive Bayes</li>
		<li>Support Vector Machine</li>
		<li>Logistic Regression<li>
	  </ul>
</p>

Since some of the data was random, we tested each classifier repeatedly. Averaging over 20 different runs, we found that the average 10-fold cross validation mean for the different classifiers were:
	  <ul>
		<li>Decision Tree: 0.673198541538</li>
		<li>Naive Bayes: 0.698055972092</li>
		<li>Support Vector Machine: 0.697729433245</li>
		<li>Logistic Regression: 0.697648221379<li>
	  </ul>
We ran this test roughly 10 more times just to ensure consistency and found that Naive Bayes was consistenly slightly better than the rest of the classifiers, at least according to cross validation statistic.
